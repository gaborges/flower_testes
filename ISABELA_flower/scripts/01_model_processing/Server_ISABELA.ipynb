{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9018ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import flwr as fl\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from flwr.common.logger import log\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    MetricsAggregationFn,\n",
    "    NDArrays,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a82cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFolders = \"../data_2019_processed/\"\n",
    "# selected features\n",
    "inputFeatures = [\"activity\",\"location\",\"day_of_week\",\"light\",\"phone_lock\",\"proximity\",\"sound\",\"time_to_next_alarm\", \"minutes_day\"]\n",
    "outputClasses = [\"awake\",\"asleep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e192aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadTestData(dataSet):\n",
    "    print(len(dataSet), \"datasets\")\n",
    "    for i in range(0,len(dataSet)):\n",
    "        print(i , \"-\", dataSet[i])\n",
    "        #print(trainingDataSet[i])\n",
    "        if(i == 0):\n",
    "            X_test = pd.read_csv(inputFolders+\"student_\"+dataSet[i]+\"_transformed.csv\")\n",
    "        else:\n",
    "            dataset = pd.read_csv(inputFolders+\"student_\"+dataSet[i]+\"_transformed.csv\")\n",
    "            X_test = pd.concat([X_test, dataset])\n",
    "    # return the dataset        \n",
    "    return X_test\n",
    "\n",
    "# one-hot encoding function\n",
    "def transform_output_nominal_class_into_one_hot_encoding(dataset):\n",
    "    # create two classes based on the single class\n",
    "    one_hot_encoded_data = pd.get_dummies(dataset['class'])\n",
    "    #print(one_hot_encoded_data)\n",
    "    dataset['awake'] = one_hot_encoded_data['awake']\n",
    "    dataset['asleep'] = one_hot_encoded_data['asleep']\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# one-hot encoding function\n",
    "def transform_output_numerical_class_into_one_hot_encoding(dataset):\n",
    "    # create two classes based on the single class\n",
    "    one_hot_encoded_data = pd.get_dummies(dataset['class'])\n",
    "    #print(one_hot_encoded_data)\n",
    "    dataset['awake'] = one_hot_encoded_data[0]\n",
    "    dataset['asleep'] = one_hot_encoded_data[1]\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def transform_data_type(dataframe):\n",
    "    \n",
    "    # transform inputs\n",
    "    for column in inputFeatures:\n",
    "        dataframe[column] = dataframe[column].astype('float32')\n",
    "    \n",
    "    # transform outputs\n",
    "    for column in outputClasses:\n",
    "        dataframe[column] = dataframe[column].astype('float32')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c41101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "        if aggregated_parameters is not None:\n",
    "            # Convert `Parameters` to `List[np.ndarray]`\n",
    "            aggregated_ndarrays: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "\n",
    "            # Save aggregated_ndarrays\n",
    "            print(f\"Saving round {server_round} aggregated_ndarrays...\")\n",
    "            np.savez(f\"round-{server_round}-weights.npz\", *aggregated_ndarrays)\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02daf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_config(server_round: int):\n",
    "    \"\"\"Return training configuration dict for each round.\n",
    "\n",
    "    Keep batch size fixed at 32, perform two rounds of training with one\n",
    "    local epoch, increase to two local epochs afterwards.\n",
    "    \"\"\"\n",
    "    print(\"onn fit\")\n",
    "    config = {\n",
    "        \"batch_size\": 32,\n",
    "        \"local_epochs\": 1,\n",
    "    }\n",
    "    return config\n",
    "\t\n",
    "\t\n",
    "def get_evaluate_fn(model):\n",
    "    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n",
    "    \n",
    "    testFolders =  [#'0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs',\n",
    "                #'0tdmm6rwW3KquQ73ATYYJ5JkpMtvbppJ0VzA2GExdA', \n",
    "                #'2cyV53lVyUtlMj0BRwilEWtYJwUiviYoL48cZBPBq0', \n",
    "                #'2J22RukYnEbKTk7t+iUVDBkorcyL5NKN6TrLe89ys', \n",
    "                #['5FLZBTVAPwdq9QezHE2sVCJIs7p+r6mCemA2gp9jATk'], #does not have the file\n",
    "                #'7EYF5I04EVqisUJCVNHlqn77UAuOmwL2Dahxd3cA', \n",
    "                #'a9Qgj8ENWrHvl9QqlXcIPKmyGMKgbfHk9Dbqon1HQP4', \n",
    "                #'ae4JJBZDycEcY8McJF+3BxyvZ1619y03BNdCxzpZTc', \n",
    "                #'Ch3u5Oaz96VSrQbf0z31X6jEIbeIekkC0mwPzCdeJ1U', \n",
    "                #'CH8f0yZkZL13zWuE9ks1CkVJRVrr+jsGdUXHrZ6YeA', \n",
    "                #'DHO1K4jgiwZJOfQTrxvKE2vn7hkjamigroGD5IaeRc', \n",
    "                #'DHPqzSqSttiba1L3BD1cptNJPjSxZ8rXxF9mY3za6WA', # does not have asleep data\n",
    "                #'dQEFscjqnIlug8Tgq97JohhSQPG2DEOWJqS86wCrcY', \n",
    "                #'HFvs2CohmhHte+AaCzFasjzegGzxZKPhkrX23iI6Xo', \n",
    "                #'jgB9E8v3Z6PKdTRTCMAijBllA9YEMtrmHbe4qsbmJWw', \n",
    "                #'JkY++R7E8myldLN3on6iQ78Ee78zCbrLuggfwGju3I', \n",
    "                #'K4SLohf+TN1Ak8Dn8iE3Lme7rEMPISfppB2sXfHX8', \n",
    "                #'oGaWetJJJEWHuvYdWYo826SQxfhCExVVQ2da8LE1Y7Q', \n",
    "                #'pyt24oiDAHsmgWMvkFKz2fn2pwcHiXchd6KchLM', \n",
    "                'PZCf1nfvhR+6fk+7+sPNMYOgb8BAMmtQtfoRS83Suc', # does not have asleep data\n",
    "                #'QUNCATForxzK0HHw46LrGOMWh0eVA8Y5XWEiUXX+cQ', \n",
    "                'rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw', \n",
    "                'RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI', \n",
    "                #'SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4'] \n",
    "                'VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is', \n",
    "                'Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw', \n",
    "                'XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA', \n",
    "                'YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw', \n",
    "                'ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM', \n",
    "                'ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY']\n",
    "    \n",
    "    # load data\n",
    "    X_test = loadTestData(testFolders)\n",
    "    # transform output to one_hot_encoding for the testing dataset\n",
    "    X_test = transform_output_nominal_class_into_one_hot_encoding(X_test)\n",
    "    # transforms the data\n",
    "    X_test = transform_data_type(X_test)\n",
    "    # selects the data to train and test\n",
    "    X_test_data = X_test[inputFeatures]\n",
    "    y_test_label = X_test[outputClasses]\n",
    " \n",
    "    # The `evaluate` function will be called after every round\n",
    "    def evaluate(\n",
    "        server_round: int, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        model.set_weights(parameters)  # Update model with the latest parameters\n",
    "        loss, accuracy = model.evaluate(X_test_data, y_test_label)\n",
    "        print(\"Testa aquiiiiiiiiiiiiiiiiiiiiiiii\", loss, accuracy)\n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "    return evaluate\n",
    "\t\n",
    "def evaluate_config(server_round: int):\n",
    "    \"\"\"Return evaluation configuration dict for each round.\n",
    "    Perform five local evaluation steps on each client (i.e., use five\n",
    "    batches) during rounds, one to three, then increase to ten local\n",
    "    evaluation steps.\n",
    "    \"\"\"\n",
    "    val_steps = 5 if server_round < 4 else 10\n",
    "    return {\"val_steps\": val_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759cdf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-26 14:00:27.099580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-26 14:00:27.157507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-26 14:00:27.157831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-26 14:00:27.158764: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-26 14:00:27.160835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-26 14:00:27.161037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-26 14:00:27.161266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-26 14:00:28.236469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-26 14:00:28.237582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-26 14:00:28.237605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-07-26 14:00:28.237923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-26 14:00:28.237976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3421 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def create_keras_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(9,)),\n",
    "      #tf.keras.layers.Dense(9, activation=tf.keras.activations.relu), \n",
    "      tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "      tf.keras.layers.Dense(8, activation=tf.keras.activations.relu),\n",
    "      tf.keras.layers.Dense(2, activation=tf.keras.activations.softmax)\n",
    "      #tf.keras.layers.Dense(2, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "# Load model and data (MobileNetV2, CIFAR-10)\n",
    "model = create_keras_model()\n",
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e94617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 datasets\n",
      "0 - PZCf1nfvhR+6fk+7+sPNMYOgb8BAMmtQtfoRS83Suc\n",
      "1 - rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw\n",
      "2 - RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI\n",
      "3 - VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is\n",
      "4 - Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw\n",
      "5 - XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA\n",
      "6 - YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw\n",
      "7 - ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM\n",
      "8 - ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY\n"
     ]
    }
   ],
   "source": [
    "# Create strategy and run server\n",
    "strategy = SaveModelStrategy( \n",
    "    # (same arguments as FedAvg here) // https://github.com/adap/flower/blob/main/examples/android/server.py\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=1.0,\n",
    "        min_fit_clients=19,\n",
    "        min_evaluate_clients=19,\n",
    "        min_available_clients=19,        \n",
    "        evaluate_fn=get_evaluate_fn(model), # evaluate_fn=None,\n",
    "        on_fit_config_fn=fit_config,\n",
    "        on_evaluate_config_fn=evaluate_config,\n",
    "        initial_parameters=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d50caff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-07-26 14:23:24,251 | app.py:148 | Starting Flower server, config: ServerConfig(num_rounds=30, round_timeout=None)\n",
      "INFO flwr 2023-07-26 14:23:24,288 | app.py:168 | Flower ECE: gRPC server running (30 rounds), SSL is disabled\n",
      "INFO flwr 2023-07-26 14:23:24,289 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-07-26 14:23:24,289 | server.py:273 | Requesting initial parameters from one random client\n",
      "INFO flwr 2023-07-26 14:23:44,884 | server.py:277 | Received initial parameters from one random client\n",
      "INFO flwr 2023-07-26 14:23:44,885 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2023-07-26 14:23:44,885 | server.py:101 | FL starting\n",
      "DEBUG flwr 2023-07-26 14:23:56,640 | server.py:218 | fit_round 1: strategy sampled 2 clients (out of 3)\n",
      "DEBUG flwr 2023-07-26 14:24:08,944 | server.py:232 | fit_round 1 received 2 results and 0 failures\n",
      "WARNING flwr 2023-07-26 14:24:09,028 | fedavg.py:243 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-07-26 14:24:09,032 | server.py:168 | evaluate_round 1: strategy sampled 19 clients (out of 19)\n",
      "DEBUG flwr 2023-07-26 14:24:44,342 | server.py:182 | evaluate_round 1 received 18 results and 1 failures\n",
      "WARNING flwr 2023-07-26 14:24:44,344 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-07-26 14:24:44,346 | server.py:218 | fit_round 2: strategy sampled 18 clients (out of 18)\n",
      "DEBUG flwr 2023-07-26 14:24:48,339 | server.py:232 | fit_round 2 received 18 results and 0 failures\n",
      "DEBUG flwr 2023-07-26 14:24:48,356 | server.py:168 | evaluate_round 2: strategy sampled 18 clients (out of 18)\n",
      "DEBUG flwr 2023-07-26 14:24:59,611 | server.py:182 | evaluate_round 2 received 18 results and 0 failures\n",
      "DEBUG flwr 2023-07-26 14:24:59,623 | server.py:218 | fit_round 3: strategy sampled 18 clients (out of 18)\n",
      "DEBUG flwr 2023-07-26 14:25:02,941 | server.py:232 | fit_round 3 received 18 results and 0 failures\n",
      "DEBUG flwr 2023-07-26 14:25:02,990 | server.py:168 | evaluate_round 3: strategy sampled 18 clients (out of 18)\n",
      "DEBUG flwr 2023-07-26 14:25:21,805 | server.py:182 | evaluate_round 3 received 18 results and 0 failures\n",
      "DEBUG flwr 2023-07-26 14:25:21,807 | server.py:218 | fit_round 4: strategy sampled 18 clients (out of 18)\n",
      "DEBUG flwr 2023-07-26 14:25:23,506 | server.py:232 | fit_round 4 received 4 results and 14 failures\n",
      "DEBUG flwr 2023-07-26 14:26:35,867 | server.py:168 | evaluate_round 4: strategy sampled 2 clients (out of 5)\n",
      "DEBUG flwr 2023-07-26 14:27:13,146 | server.py:182 | evaluate_round 4 received 2 results and 0 failures\n",
      "DEBUG flwr 2023-07-26 14:27:13,153 | server.py:218 | fit_round 5: strategy sampled 19 clients (out of 19)\n",
      "DEBUG flwr 2023-07-26 14:27:20,228 | server.py:232 | fit_round 5 received 19 results and 0 failures\n",
      "DEBUG flwr 2023-07-26 14:27:20,251 | server.py:168 | evaluate_round 5: strategy sampled 19 clients (out of 19)\n",
      "DEBUG flwr 2023-07-26 14:27:55,983 | server.py:182 | evaluate_round 5 received 19 results and 0 failures\n",
      "DEBUG flwr 2023-07-26 14:27:55,984 | server.py:218 | fit_round 6: strategy sampled 19 clients (out of 19)\n",
      "DEBUG flwr 2023-07-26 14:28:00,668 | server.py:232 | fit_round 6 received 19 results and 0 failures\n",
      "DEBUG flwr 2023-07-26 14:28:00,676 | server.py:168 | evaluate_round 6: strategy sampled 19 clients (out of 19)\n",
      "DEBUG flwr 2023-07-26 14:43:28,710 | server.py:182 | evaluate_round 6 received 18 results and 1 failures\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start Flower server\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0:8099\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mServerConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/server/app.py:176\u001b[0m, in \u001b[0;36mstart_server\u001b[0;34m(server_address, server, config, strategy, client_manager, grpc_max_message_length, certificates)\u001b[0m\n\u001b[1;32m    168\u001b[0m log(\n\u001b[1;32m    169\u001b[0m     INFO,\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlower ECE: gRPC server running (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m rounds), SSL is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    171\u001b[0m     initialized_config\u001b[38;5;241m.\u001b[39mnum_rounds,\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m certificates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    173\u001b[0m )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43m_fl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitialized_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitialized_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Stop the gRPC server\u001b[39;00m\n\u001b[1;32m    182\u001b[0m grpc_server\u001b[38;5;241m.\u001b[39mstop(grace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/server/app.py:217\u001b[0m, in \u001b[0;36m_fl\u001b[0;34m(server, config)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fl\u001b[39m(\n\u001b[1;32m    213\u001b[0m     server: Server,\n\u001b[1;32m    214\u001b[0m     config: ServerConfig,\n\u001b[1;32m    215\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m History:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# Fit model\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     hist \u001b[38;5;241m=\u001b[39m \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     log(INFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapp_fit: losses_distributed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(hist\u001b[38;5;241m.\u001b[39mlosses_distributed))\n\u001b[1;32m    219\u001b[0m     log(INFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapp_fit: metrics_distributed_fit \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(hist\u001b[38;5;241m.\u001b[39mmetrics_distributed_fit))\n",
      "File \u001b[0;32m~/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/server/server.py:106\u001b[0m, in \u001b[0;36mServer.fit\u001b[0;34m(self, num_rounds, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m start_time \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m current_round \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_rounds \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Train model and replace previous global model\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     res_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_round\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res_fit:\n\u001b[1;32m    108\u001b[0m         parameters_prime, fit_metrics, _ \u001b[38;5;241m=\u001b[39m res_fit  \u001b[38;5;66;03m# fit_metrics_aggregated\u001b[39;00m\n",
      "File \u001b[0;32m~/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/server/server.py:209\u001b[0m, in \u001b[0;36mServer.fit_round\u001b[0;34m(self, server_round, timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform a single round of federated averaging.\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Get clients and their respective instructions from strategy\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m client_instructions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m client_instructions:\n\u001b[1;32m    216\u001b[0m     log(INFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_round \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: no clients selected, cancel\u001b[39m\u001b[38;5;124m\"\u001b[39m, server_round)\n",
      "File \u001b[0;32m~/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/server/strategy/fedavg.py:184\u001b[0m, in \u001b[0;36mFedAvg.configure_fit\u001b[0;34m(self, server_round, parameters, client_manager)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Sample clients\u001b[39;00m\n\u001b[1;32m    181\u001b[0m sample_size, min_num_clients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_fit_clients(\n\u001b[1;32m    182\u001b[0m     client_manager\u001b[38;5;241m.\u001b[39mnum_available()\n\u001b[1;32m    183\u001b[0m )\n\u001b[0;32m--> 184\u001b[0m clients \u001b[38;5;241m=\u001b[39m \u001b[43mclient_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_num_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_num_clients\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Return client/config pairs\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(client, fit_ins) \u001b[38;5;28;01mfor\u001b[39;00m client \u001b[38;5;129;01min\u001b[39;00m clients]\n",
      "File \u001b[0;32m~/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/server/client_manager.py:180\u001b[0m, in \u001b[0;36mSimpleClientManager.sample\u001b[0;34m(self, num_clients, min_num_clients, criterion)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_num_clients \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     min_num_clients \u001b[38;5;241m=\u001b[39m num_clients\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_num_clients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Sample clients which meet the criterion\u001b[39;00m\n\u001b[1;32m    182\u001b[0m available_cids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients)\n",
      "File \u001b[0;32m~/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/server/client_manager.py:125\u001b[0m, in \u001b[0;36mSimpleClientManager.wait_for\u001b[0;34m(self, num_clients, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait until at least `num_clients` are available.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03mBlocks until the requested number of clients is available or until a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03msuccess : bool\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cv:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclients\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/threading.py:347\u001b[0m, in \u001b[0;36mCondition.wait_for\u001b[0;34m(self, predicate, timeout)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m waittime \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    346\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaittime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     result \u001b[38;5;241m=\u001b[39m predicate()\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start Flower server\n",
    "fl.server.start_server(\n",
    "    server_address=\"0.0.0.0:8099\",\n",
    "    config=fl.server.ServerConfig(num_rounds=30),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1399022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
